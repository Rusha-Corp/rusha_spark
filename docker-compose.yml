services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: postgres
    volumes:
      - $HOME/postgresql/dbs/openalex:/var/lib/postgresql/data
    networks:
      - spark-container-network-dbt
    hostname: postgres
    restart: always
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: 4G
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s # Delay to ensure the database is fully ready

  metastore:
    image: europe-west1-docker.pkg.dev/owa-gemini/docker-registry/hive_metastore:v11
    ports:
      - 9083:9083
    networks:
      - spark-container-network-dbt
    volumes:
      - /tmp:/tmp
    environment:
      - SERVICE_NAME=metastore
      - DB_DRIVER=postgres    
      - VERBOSE=true
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
      - HADOOP_CLIENT_OPTS=-Xmx1G -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/postgres -Djavax.jdo.option.ConnectionUserName=postgres -Djavax.jdo.option.ConnectionPassword=password
    restart: always
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: 8G
    depends_on:
      postgres:
        condition: service_healthy

  spark-thrift-server:
    build: 
      context: .
      dockerfile: Dockerfile
    ports:
      - "4041:4040"
    hostname: spark-thrift-server
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
      - AWS_SESSION_TOKEN=${AWS_SESSION_TOKEN}
      - AWS_REGION=${AWS_DEFAULT_REGION}
      - SPARK_WAREHOUSE_DIR=${WAREHOUSE}
      - METASTORE_URIS=${METASTORE_URIS}
      - SPARK_LOG_DIR=/tmp/spark-events
      - SPARK_DRIVER_HOST=spark-thrift-server
      - SPARK_EXECUTOR_CORES=8
      - SPARK_EXECUTOR_MEMORY=64G
    volumes:
      - /tmp:/tmp
    networks:
      - spark-container-network-dbt
    restart: always
    entrypoint: ["/bin/bash", "-c", "/start_thrift_server.sh"]
    deploy:
      resources:
        limits:
          cpus: "8"
          memory: 32G
    healthcheck:
      test: ["CMD-SHELL", "curl -s thrift://spark-thrift-server:10000 | grep -q Thrift"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  spark-history-server:
    build: 
      context: .
      dockerfile: Dockerfile
    ports:
      - "18080:18080"
    hostname: spark-history-server
    volumes:
      - /tmp:/tmp
    networks:
      - spark-container-network-dbt
    # depends_on:
    #   spark-thrift-server:
    #     condition: service_healthy  
    restart: always
    entrypoint: /bin/bash -c /start_history_server.sh
    environment:
      - SPARK_LOG_DIR=/tmp/spark-events
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: 4G

  hue:
    image: gethue/hue:latest
    hostname: hue
    container_name: hue
    dns: 8.8.8.8
    depends_on:
      - spark-thrift-server
      - postgres
    ports:
    - "8888:8888"
    volumes:
      - ./conf:/usr/share/hue/desktop/conf
    networks:
      - spark-container-network-dbt
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: 4G

  ml:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
      - AWS_REGION=${AWS_DEFAULT_REGION}
    ports:
      - "5000:5000"
    command: mlflow server --backend-store-uri postgresql://postgres:password@postgres:5432/postgres --default-artifact-root ${ARTIFACTS_ROOT} --host 0.0.0.0 --port 5000
    volumes:
      - /tmp:/tmp
      - ./mlflow:/mlflow
    networks:
      - spark-container-network-dbt
    restart: always
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: 4G

  iceberg-rest:
    image: apache/iceberg-rest-fixture
    container_name: iceberg-rest
    networks:
      - spark-container-network-dbt
    ports:
      - 8181:8181
    restart: always
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_DEFAULT_REGION}
      - CATALOG_WAREHOUSE=${WAREHOUSE}/iceberg_catalog
      - CATALOG_IO__IMPL=org.apache.iceberg.aws.s3.S3FileIO
      - CATALOG_S3_ENDPOINT=s3.${AWS_DEFAULT_REGION}.amazonaws.com
      - CATALOG_JDBC__IMPL=org.postgresql.Driver
      - CATALOG_JDBC__URL=jdbc:postgresql://postgres:5432/postgres
      - CATALOG_JDBC__USER=postgres
      - CATALOG_JDBC__PASSWORD=password

networks:
  spark-container-network-dbt:
    external: true
